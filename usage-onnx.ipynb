{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "33e000ed-6ed7-42ae-b723-a2bdf1435764",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import onnxruntime as rt\n",
    "from onnxruntime import InferenceSession\n",
    "# ReferenceEvaluator from onnx.reference is not used in the original\n",
    "# so it's commented out for simplicity.\n",
    "# from onnx.reference import ReferenceEvaluator \n",
    "import joblib\n",
    "from pythainlp.corpus import thai_orst_words\n",
    "import numpy as np\n",
    "\n",
    "# list_word_use=[i for i in list_word if \" \" not in i]\n",
    "\n",
    "\n",
    "class FastTextEncoder:\n",
    "    \"\"\"\n",
    "    A class to load pre-trained FastText-like word embeddings, \n",
    "    compute word and sentence vectors, and interact with an ONNX \n",
    "    model for nearest neighbor suggestions.\n",
    "    \"\"\"\n",
    "\n",
    "    # --- Initialization and Data Loading ---\n",
    "    \n",
    "    def __init__(self, model_dir, nn_model_path, words_list_path, bucket=2000000, nb_words=2000000, minn=5, maxn=5):\n",
    "        \"\"\"\n",
    "        Initializes the FastTextEncoder, loading embeddings, vocabulary, \n",
    "        nearest neighbor model, and suggestion words list.\n",
    "\n",
    "        Args:\n",
    "            model_dir (str): Directory containing 'embeddings.npy' and 'vocabulary.txt'.\n",
    "            nn_model_path (str): Path to the ONNX nearest neighbors model.\n",
    "            words_list_path (str): Path to the joblib file containing the list of words for suggestions.\n",
    "            bucket (int): The size of the hash bucket for subword hashing.\n",
    "            nb_words (int): The number of words in the vocabulary (used as an offset for subword indices).\n",
    "            minn (int): Minimum character length for subwords.\n",
    "            maxn (int): Maximum character length for subwords.\n",
    "        \"\"\"\n",
    "        self.model_dir = model_dir\n",
    "        self.nn_model_path = nn_model_path\n",
    "        self.bucket = bucket\n",
    "        self.nb_words = nb_words\n",
    "        self.minn = minn\n",
    "        self.maxn = maxn\n",
    "\n",
    "        # Load data and models\n",
    "        self.vocabulary, self.embeddings = self._load_embeddings()\n",
    "        self.words_for_suggestion = self._load_suggestion_words(words_list_path)\n",
    "        self.nn_session = self._load_onnx_session(nn_model_path)\n",
    "        self.embedding_dim = self.embeddings.shape[1]\n",
    "\n",
    "    def _load_embeddings(self):\n",
    "        \"\"\"Loads embeddings matrix and vocabulary list.\"\"\"\n",
    "        print(f\"Loading embeddings from {self.model_dir}...\")\n",
    "        input_matrix = np.load(os.path.join(self.model_dir, \"embeddings.npy\"))\n",
    "        words = []\n",
    "        vocab_path = os.path.join(self.model_dir, \"vocabulary.txt\")\n",
    "        with open(vocab_path, \"r\", encoding='utf-8') as f:\n",
    "            for line in f.readlines():\n",
    "                words.append(line.rstrip())\n",
    "        print(\"Embeddings loaded successfully.\")\n",
    "        return words, input_matrix\n",
    "\n",
    "    def _load_suggestion_words(self, words_list_path):\n",
    "        \"\"\"Loads the list of words used for suggestions.\"\"\"\n",
    "        print(f\"Loading suggestion words from {words_list_path}...\")\n",
    "        list_word=sorted(list(thai_orst_words()))\n",
    "        words=list_word\n",
    "        words = np.array(list(words))\n",
    "        # words = words\n",
    "        print(\"Suggestion words loaded successfully.\")\n",
    "        return words\n",
    "\n",
    "    def _load_onnx_session(self, onnx_path):\n",
    "        \"\"\"Loads the ONNX inference session.\"\"\"\n",
    "        print(f\"Loading ONNX model from {onnx_path}...\")\n",
    "        # Note: Using providers=[\"CPUExecutionProvider\"] for platform independence\n",
    "        sess = rt.InferenceSession(onnx_path, providers=[\"CPUExecutionProvider\"])\n",
    "        print(\"ONNX session created successfully.\")\n",
    "        return sess\n",
    "\n",
    "    # --- Helper Methods for Encoding ---\n",
    "\n",
    "    def _get_hash(self, subword):\n",
    "        \"\"\"Computes the FastText-like hash for a subword.\"\"\"\n",
    "        h = 2166136261  # FNV-1a basis\n",
    "        for c in subword:\n",
    "            c_ord = ord(c) % 2**8\n",
    "            h = (h ^ c_ord) % 2**32\n",
    "            h = (h * 16777619) % 2**32  # FNV-1a prime\n",
    "        return h % self.bucket + self.nb_words\n",
    "\n",
    "    def _get_subwords(self, word):\n",
    "        \"\"\"Extracts subwords and their corresponding indices for a given word.\"\"\"\n",
    "        _word = \"<\" + word + \">\"\n",
    "        _subwords = []\n",
    "        _subword_ids = []\n",
    "\n",
    "        # 1. Check for the word in vocabulary (full word is the first subword)\n",
    "        if word in self.vocabulary:\n",
    "            _subwords.append(word)\n",
    "            _subword_ids.append(self.vocabulary.index(word))\n",
    "            if word == \"</s>\":\n",
    "                return _subwords, np.array(_subword_ids)\n",
    "\n",
    "        # 2. Extract n-grams (subwords) and get their hash indices\n",
    "        for ngram_start in range(0, len(_word)):\n",
    "            for ngram_length in range(self.minn, self.maxn + 1):\n",
    "                if ngram_start + ngram_length <= len(_word):\n",
    "                    _candidate_subword = _word[ngram_start:ngram_start + ngram_length]\n",
    "                    # Only append if not already included (e.g., as the full word)\n",
    "                    if _candidate_subword not in _subwords: \n",
    "                        _subwords.append(_candidate_subword)\n",
    "                        _subword_ids.append(self._get_hash(_candidate_subword))\n",
    "\n",
    "        return _subwords, np.array(_subword_ids)\n",
    "\n",
    "    def get_word_vector(self, word):\n",
    "        \"\"\"Computes the normalized vector for a single word.\"\"\"\n",
    "        # subword_ids[1] contains the array of indices for the word and its subwords\n",
    "        subword_ids = self._get_subwords(word)[1]\n",
    "        \n",
    "        # Check if the array of subword indices is empty\n",
    "        if subword_ids.size == 0:\n",
    "            # Return a 300-dimensional zero vector if no word/subword is found.\n",
    "            return np.zeros(self.embedding_dim)\n",
    "\n",
    "        # Compute the mean of the embeddings for all subword indices\n",
    "        vector = np.mean([self.embeddings[s] for s in subword_ids], axis=0)\n",
    "        \n",
    "        # Normalize the vector\n",
    "        norm = np.linalg.norm(vector)\n",
    "        if norm > 0:\n",
    "            vector /= norm\n",
    "            \n",
    "        return vector\n",
    "\n",
    "    def _tokenize(self, sentence):\n",
    "        \"\"\"Tokenizes a sentence based on whitespace.\"\"\"\n",
    "        tokens = []\n",
    "        word = \"\"\n",
    "        for c in sentence:\n",
    "            if c in [' ', '\\n', '\\r', '\\t', '\\v', '\\f', '\\0']:\n",
    "                if word:\n",
    "                    tokens.append(word)\n",
    "                    word = \"\"\n",
    "                if c == '\\n':\n",
    "                    tokens.append(\"</s>\")\n",
    "            else:\n",
    "                word += c\n",
    "        if word:\n",
    "            tokens.append(word)\n",
    "        return tokens\n",
    "\n",
    "    def get_sentence_vector(self, line):\n",
    "        \"\"\"Computes the mean vector for a sentence.\"\"\"\n",
    "        tokens = self._tokenize(line)\n",
    "        vectors = []\n",
    "        for t in tokens:\n",
    "            # get_word_vector already handles normalization, so no need to do it again here\n",
    "            vec = self.get_word_vector(t)\n",
    "            vectors.append(vec)\n",
    "            \n",
    "        # If the sentence was empty and resulted in no vectors, return a zero vector\n",
    "        if not vectors:\n",
    "            return np.zeros(self.embedding_dim)\n",
    "\n",
    "        return np.mean(vectors, axis=0)\n",
    "\n",
    "    # --- Nearest Neighbor Method ---\n",
    "\n",
    "    def get_word_suggestion(self, list_word):\n",
    "        \"\"\"\n",
    "        Queries the ONNX model to find the nearest neighbor word(s) \n",
    "        for the given word or list of words.\n",
    "\n",
    "        Args:\n",
    "            list_word (str or list of str): A single word or a list of words \n",
    "                                            to get suggestions for.\n",
    "\n",
    "        Returns:\n",
    "            str or list of str: The nearest neighbor word(s) from the \n",
    "                                pre-loaded suggestion list.\n",
    "        \"\"\"\n",
    "        if isinstance(list_word, str):\n",
    "            input_words = [list_word]\n",
    "            return_single = True\n",
    "        else:\n",
    "            input_words = list_word\n",
    "            return_single = False\n",
    "            \n",
    "        # Compute sentence vector for each input word/phrase\n",
    "        # The original code's `get_sentence_vector(' '.join(list(word)))` seems \n",
    "        # intended to treat a list of characters/tokens as a sentence. \n",
    "        # I'll stick to a more standard usage: treat each item in `input_words` \n",
    "        # as a separate phrase/word to encode.\n",
    "        word_input_vecs = [self.get_sentence_vector(' '.join(list(word))) for word in input_words]\n",
    "\n",
    "        # Convert to numpy array for ONNX input (ensure float32)\n",
    "        input_data = np.array(word_input_vecs, dtype=np.float32)\n",
    "\n",
    "        # Run ONNX inference\n",
    "        indices = self.nn_session.run(None, {\"X\": input_data})[0]\n",
    "        \n",
    "        # Look up suggestions\n",
    "        suggestions = [self.words_for_suggestion[i].tolist() for i in indices]\n",
    "        \n",
    "        return suggestions[0] if return_single else suggestions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3672c995-2cf1-4be4-aa9b-5ffafa37340c",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_DIRECTORY = 'model/'\n",
    "ONNX_PATH = \"nearest_neighbors.onnx\"\n",
    "WORDS_LIST_PATH = 'words.joblib'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0cfd3258-8620-4501-b6b6-59b69afdc2ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading embeddings from model/...\n",
      "Embeddings loaded successfully.\n",
      "Loading suggestion words from words.joblib...\n",
      "Suggestion words loaded successfully.\n",
      "Loading ONNX model from nearest_neighbors.onnx...\n",
      "ONNX session created successfully.\n"
     ]
    }
   ],
   "source": [
    "encoder = FastTextEncoder(\n",
    "    model_dir=MODEL_DIRECTORY, \n",
    "    nn_model_path=ONNX_PATH, \n",
    "    words_list_path=WORDS_LIST_PATH\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bf37ae7e-65d5-4f14-8264-a27292112f70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector for '['โรงเรีย', 'คมดี']' (first 5 values): [['โรงเรียน', 'ระเรียง', 'โรงเรียนประจำ', 'เรียบเรียง', 'ระเดียง'], ['คนดีผีคุ้ม', 'มีดคอม้า', 'คดี', 'มีดสองคม', 'มูลคดี']]\n"
     ]
    }
   ],
   "source": [
    "word = [\"โรงเรีย\",\"คมดี\"]\n",
    "word_vector = encoder.get_word_suggestion(word)\n",
    "print(f\"Vector for '{word}' (first 5 values): {word_vector[:5]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "db991d02-b084-4100-aa6e-839a72a96c3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['โรงเรียน', 'ระเรียง', 'โรงเรียนประจำ', 'เรียบเรียง', 'ระเดียง'],\n",
       " ['คนดีผีคุ้ม', 'มีดคอม้า', 'คดี', 'มีดสองคม', 'มูลคดี']]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4c3f0dd7-9d58-4d3b-bb5b-3f957569dd6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector for 'โรงเรีย' (first 5 values): ['โรงเรียน', 'ระเรียง', 'โรงเรียนประจำ', 'เรียบเรียง', 'ระเดียง']\n"
     ]
    }
   ],
   "source": [
    "word = \"โรงเรีย\"\n",
    "word_vector = encoder.get_word_suggestion(word)\n",
    "print(f\"Vector for '{word}' (first 5 values): {word_vector}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a34a26a-cdbf-4ffc-a306-43be2805210c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
