{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e7778295-3cdd-4190-8c65-bb5b2017acb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import onnxruntime as rt\n",
    "from onnxruntime import InferenceSession\n",
    "from onnx.reference import ReferenceEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "92c7f636-92f9-4690-98ae-b05c78637b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_embeddings(output_dir):\n",
    "  input_matrix = np.load(os.path.join(output_dir, \"embeddings.npy\"))\n",
    "  words = []\n",
    "  with open(os.path.join(output_dir, \"vocabulary.txt\"), \"r\", encoding='utf-8') as f:\n",
    "    for line in f.readlines():\n",
    "      words.append(line.rstrip())\n",
    "  return words, input_matrix\n",
    "\n",
    "vocabulary, embeddings = load_embeddings('model/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "72f00eff-c4f7-44ac-b36c-fd3ec77f0bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hash(subword, bucket=2000000, nb_words=2000000):\n",
    "  h = 2166136261\n",
    "  for c in subword:\n",
    "    c = ord(c) % 2**8\n",
    "    h = (h ^ c) % 2**32\n",
    "    h = (h * 16777619) % 2**32\n",
    "  return h % bucket + nb_words\n",
    "def get_subwords(word, vocabulary, minn=5, maxn=5):\n",
    "  _word = \"<\" + word + \">\"\n",
    "  _subwords = []\n",
    "  _subword_ids = []\n",
    "  if word in vocabulary:\n",
    "    _subwords.append(word)\n",
    "    _subword_ids.append(vocabulary.index(word))\n",
    "    if word == \"</s>\":\n",
    "      return _subwords, np.array(_subword_ids)\n",
    "  for ngram_start in range(0, len(_word)):\n",
    "    for ngram_length in range(minn, maxn+1):\n",
    "      if ngram_start+ngram_length <= len(_word):\n",
    "        _candidate_subword = _word[ngram_start:ngram_start+ngram_length]\n",
    "        if _candidate_subword not in _subwords:\n",
    "          _subwords.append(_candidate_subword)\n",
    "          _subword_ids.append(get_hash(_candidate_subword))\n",
    "  return _subwords, np.array(_subword_ids)\n",
    "def get_word_vector(word, vocabulary, embeddings):\n",
    "  # subwords[1] contains the array of indices for the word and its subwords\n",
    "  subword_ids = get_subwords(word, vocabulary)[1]\n",
    "  \n",
    "  # Check if the array of subword indices is empty\n",
    "  if subword_ids.size == 0:\n",
    "    # üí• FIX: If no word/subword is found, return a 300-dimensional zero vector.\n",
    "    # This ensures that all elements appended to the 'vectors' list have the same shape.\n",
    "    embedding_dim = embeddings.shape[1] # This should be 300\n",
    "    return np.zeros(embedding_dim)\n",
    "\n",
    "  # Otherwise, compute the mean as before\n",
    "  return np.mean([embeddings[s] for s in subword_ids], axis=0)\n",
    "def tokenize(sentence):\n",
    "  tokens = []\n",
    "  word = \"\"\n",
    "  for c in sentence:\n",
    "    if c in [' ', '\\n', '\\r', '\\t', '\\v', '\\f', '\\0']:\n",
    "      if word:\n",
    "        tokens.append(word)\n",
    "        word = \"\"\n",
    "      if c == '\\n':\n",
    "        tokens.append(\"</s>\")\n",
    "    else:\n",
    "      word += c\n",
    "  if word:\n",
    "    tokens.append(word)\n",
    "  return tokens\n",
    "\n",
    "def get_sentence_vector(line):\n",
    "    tokens = tokenize(line)\n",
    "    vectors = []\n",
    "    for t in tokens:\n",
    "        vec = get_word_vector(t, vocabulary, embeddings)\n",
    "        norm = np.linalg.norm(vec)\n",
    "        if norm > 0:\n",
    "            vec /= norm\n",
    "        vectors.append(vec)\n",
    "    return np.mean(vectors, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "775dac40-e111-467f-a715-fab30454c722",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pythainlp.corpus import thai_orst_words\n",
    "import numpy as np\n",
    "list_word=sorted(list(thai_orst_words()))\n",
    "# list_word_use=[i for i in list_word if \" \" not in i]\n",
    "words=list_word\n",
    "words = np.array(list(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "46c2b89c-6597-4193-8a87-08044cee4918",
   "metadata": {},
   "outputs": [],
   "source": [
    "onnx_path = \"nearest_neighbors.onnx\" # Path where you saved the model\n",
    "\n",
    "# 1. Load the model using InferenceSession\n",
    "sess = rt.InferenceSession(onnx_path, providers=[\"CPUExecutionProvider\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7a6a0667-90e0-4ee1-b165-433e7b4b885e",
   "metadata": {},
   "outputs": [],
   "source": [
    "words_input = ['‡∏Ñ‡∏î‡∏î‡∏µ', '‡∏™‡∏ß‡∏±‡∏î‡∏µ', 'v‡∏≠‡∏≠‡∏Å‡πÄ‡∏•‡∏≠‡∏£‡πå', '‡∏õ‡∏∞‡πÄ‡∏ó‡∏®‡πÑ‡∏ó‡∏¢‡∏¢', '‡∏≠‡∏£‡∏≠‡∏¢']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "23e38c54-79f5-4770-89fd-adb9370ec10d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‡∏Ñ‡∏î‡∏î‡∏µ \n",
      "---> ['‡∏Ñ‡∏î‡∏µ' '‡∏Ñ‡∏î‡∏µ‡∏î‡∏≥' '‡∏Ñ‡∏ô‡∏î‡∏µ' '‡∏Ñ‡∏î‡∏µ‡πÅ‡∏î‡∏á' '‡∏î‡∏µ‡∏î‡∏•‡∏π‡∏Å‡∏Ñ‡∏¥‡∏î']\n",
      "‡∏™‡∏ß‡∏±‡∏î‡∏µ \n",
      "---> ['‡∏ß‡∏™‡∏ß‡∏±‡∏î‡∏î‡∏µ' '‡∏™‡∏ß‡∏±‡∏™‡∏î‡∏µ' '‡∏î‡∏ö‡∏±‡∏™‡∏ß‡∏µ' '‡∏ß‡∏¥‡∏î‡∏±‡∏™‡∏î‡∏µ' '‡∏™‡∏£‡∏±‡∏™‡∏ß‡∏î‡∏µ']\n",
      "v‡∏≠‡∏≠‡∏Å‡πÄ‡∏•‡∏≠‡∏£‡πå \n",
      "---> ['‡∏≠‡∏≠‡∏£‡πå‡πÅ‡∏Å‡∏ô' '‡∏≠‡∏≤‡∏£‡πå‡∏Å‡∏≠‡∏ô' '‡∏≠‡∏≠‡∏Å‡πÄ‡∏ö‡∏≠‡∏£‡πå' '‡πÄ‡∏≠‡∏ó‡∏¥‡∏•‡πÅ‡∏≠‡∏•‡∏Å‡∏≠‡∏Æ‡∏≠‡∏•‡πå' '‡∏Ñ‡∏•‡∏≠‡πÇ‡∏£‡∏ü‡∏≠‡∏£‡πå‡∏°']\n",
      "‡∏õ‡∏∞‡πÄ‡∏ó‡∏®‡πÑ‡∏ó‡∏¢‡∏¢ \n",
      "---> ['‡∏Å‡∏£‡∏∞‡πÄ‡∏ó‡∏µ‡∏¢‡∏°‡πÇ‡∏ó‡∏ô' '‡∏Å‡∏∞‡πÄ‡∏ó‡∏¢' '‡∏õ‡∏£‡∏∞‡πÇ‡∏°‡∏ó‡∏¢‡πå' '‡∏Ñ‡∏∏‡∏¢‡∏´‡∏õ‡∏£‡∏∞‡πÄ‡∏ó‡∏®' '‡∏ó‡∏£‡∏á‡∏Å‡∏£‡∏∞‡πÄ‡∏ó‡∏µ‡∏¢‡∏°']\n",
      "‡∏≠‡∏£‡∏≠‡∏¢ \n",
      "---> ['‡∏£‡∏≠‡∏¢' '‡∏≠‡∏£‡πà‡∏≠‡∏¢' '‡∏£‡πà‡∏≠‡∏á‡∏£‡∏≠‡∏¢' '‡πÄ‡∏≠‡∏£‡πá‡∏î‡∏≠‡∏£‡πà‡∏≠‡∏¢' '‡∏¢‡πâ‡∏≠‡∏ô‡∏£‡∏≠‡∏¢']\n"
     ]
    }
   ],
   "source": [
    "word_input_vec = [get_sentence_vector(' '.join(list(word))) for word in words_input]\n",
    "indices = sess.run(None, {\"X\": word_input_vec})[0] # n_neighbors is 5\n",
    "suggestion = words[indices]\n",
    "\n",
    "for w, s in zip(words_input, suggestion):\n",
    "    print(f'{w} \\n---> {s}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "64860e15-f253-4c1c-85ac-26f8c747a543",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('‡∏Ñ‡∏î‡∏î‡∏µ',\n",
       "  array(['‡∏Ñ‡∏î‡∏µ', '‡∏Ñ‡∏î‡∏µ‡∏î‡∏≥', '‡∏Ñ‡∏ô‡∏î‡∏µ', '‡∏Ñ‡∏î‡∏µ‡πÅ‡∏î‡∏á', '‡∏î‡∏µ‡∏î‡∏•‡∏π‡∏Å‡∏Ñ‡∏¥‡∏î'], dtype='<U50')),\n",
       " ('‡∏™‡∏ß‡∏±‡∏î‡∏µ',\n",
       "  array(['‡∏ß‡∏™‡∏ß‡∏±‡∏î‡∏î‡∏µ', '‡∏™‡∏ß‡∏±‡∏™‡∏î‡∏µ', '‡∏î‡∏ö‡∏±‡∏™‡∏ß‡∏µ', '‡∏ß‡∏¥‡∏î‡∏±‡∏™‡∏î‡∏µ', '‡∏™‡∏£‡∏±‡∏™‡∏ß‡∏î‡∏µ'], dtype='<U50')),\n",
       " ('v‡∏≠‡∏≠‡∏Å‡πÄ‡∏•‡∏≠‡∏£‡πå',\n",
       "  array(['‡∏≠‡∏≠‡∏£‡πå‡πÅ‡∏Å‡∏ô', '‡∏≠‡∏≤‡∏£‡πå‡∏Å‡∏≠‡∏ô', '‡∏≠‡∏≠‡∏Å‡πÄ‡∏ö‡∏≠‡∏£‡πå', '‡πÄ‡∏≠‡∏ó‡∏¥‡∏•‡πÅ‡∏≠‡∏•‡∏Å‡∏≠‡∏Æ‡∏≠‡∏•‡πå', '‡∏Ñ‡∏•‡∏≠‡πÇ‡∏£‡∏ü‡∏≠‡∏£‡πå‡∏°'],\n",
       "        dtype='<U50')),\n",
       " ('‡∏õ‡∏∞‡πÄ‡∏ó‡∏®‡πÑ‡∏ó‡∏¢‡∏¢',\n",
       "  array(['‡∏Å‡∏£‡∏∞‡πÄ‡∏ó‡∏µ‡∏¢‡∏°‡πÇ‡∏ó‡∏ô', '‡∏Å‡∏∞‡πÄ‡∏ó‡∏¢', '‡∏õ‡∏£‡∏∞‡πÇ‡∏°‡∏ó‡∏¢‡πå', '‡∏Ñ‡∏∏‡∏¢‡∏´‡∏õ‡∏£‡∏∞‡πÄ‡∏ó‡∏®', '‡∏ó‡∏£‡∏á‡∏Å‡∏£‡∏∞‡πÄ‡∏ó‡∏µ‡∏¢‡∏°'],\n",
       "        dtype='<U50')),\n",
       " ('‡∏≠‡∏£‡∏≠‡∏¢',\n",
       "  array(['‡∏£‡∏≠‡∏¢', '‡∏≠‡∏£‡πà‡∏≠‡∏¢', '‡∏£‡πà‡∏≠‡∏á‡∏£‡∏≠‡∏¢', '‡πÄ‡∏≠‡∏£‡πá‡∏î‡∏≠‡∏£‡πà‡∏≠‡∏¢', '‡∏¢‡πâ‡∏≠‡∏ô‡∏£‡∏≠‡∏¢'], dtype='<U50'))]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(zip(words_input, suggestion))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1dec92cd-86e4-4b50-b545-fe0ec823003b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['‡∏Ñ‡∏î‡∏µ', '‡∏Ñ‡∏î‡∏µ‡∏î‡∏≥', '‡∏Ñ‡∏ô‡∏î‡∏µ', '‡∏Ñ‡∏î‡∏µ‡πÅ‡∏î‡∏á', '‡∏î‡∏µ‡∏î‡∏•‡∏π‡∏Å‡∏Ñ‡∏¥‡∏î'],\n",
       "       ['‡∏ß‡∏™‡∏ß‡∏±‡∏î‡∏î‡∏µ', '‡∏™‡∏ß‡∏±‡∏™‡∏î‡∏µ', '‡∏î‡∏ö‡∏±‡∏™‡∏ß‡∏µ', '‡∏ß‡∏¥‡∏î‡∏±‡∏™‡∏î‡∏µ', '‡∏™‡∏£‡∏±‡∏™‡∏ß‡∏î‡∏µ'],\n",
       "       ['‡∏≠‡∏≠‡∏£‡πå‡πÅ‡∏Å‡∏ô', '‡∏≠‡∏≤‡∏£‡πå‡∏Å‡∏≠‡∏ô', '‡∏≠‡∏≠‡∏Å‡πÄ‡∏ö‡∏≠‡∏£‡πå', '‡πÄ‡∏≠‡∏ó‡∏¥‡∏•‡πÅ‡∏≠‡∏•‡∏Å‡∏≠‡∏Æ‡∏≠‡∏•‡πå', '‡∏Ñ‡∏•‡∏≠‡πÇ‡∏£‡∏ü‡∏≠‡∏£‡πå‡∏°'],\n",
       "       ['‡∏Å‡∏£‡∏∞‡πÄ‡∏ó‡∏µ‡∏¢‡∏°‡πÇ‡∏ó‡∏ô', '‡∏Å‡∏∞‡πÄ‡∏ó‡∏¢', '‡∏õ‡∏£‡∏∞‡πÇ‡∏°‡∏ó‡∏¢‡πå', '‡∏Ñ‡∏∏‡∏¢‡∏´‡∏õ‡∏£‡∏∞‡πÄ‡∏ó‡∏®', '‡∏ó‡∏£‡∏á‡∏Å‡∏£‡∏∞‡πÄ‡∏ó‡∏µ‡∏¢‡∏°'],\n",
       "       ['‡∏£‡∏≠‡∏¢', '‡∏≠‡∏£‡πà‡∏≠‡∏¢', '‡∏£‡πà‡∏≠‡∏á‡∏£‡∏≠‡∏¢', '‡πÄ‡∏≠‡∏£‡πá‡∏î‡∏≠‡∏£‡πà‡∏≠‡∏¢', '‡∏¢‡πâ‡∏≠‡∏ô‡∏£‡∏≠‡∏¢']], dtype='<U50')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "suggestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75c0f90e-2ef8-4b4d-abef-541c41165233",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
